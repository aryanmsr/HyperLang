{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperLang Sample Concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample concept, we will generate transcripts in Spanish given the country of origin and scenario. For now, we manually specify the country and scenario in this notebook, but, in the future we can create more\n",
    "specific parameters (e.g. difficulty, number of speakers in conversation) that will be inputted by the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryanmishra/Desktop/HyperLang/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/aryanmishra/Desktop/HyperLang/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Handles LLM interactions using Hugging Face's API for text generation.\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import Dict, Any, AsyncGenerator\n",
    "import asyncio\n",
    "import config\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class PromptHandler:\n",
    "   \"\"\"\n",
    "   Class to handle loading and formatting of prompt templates.\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self, template_path: str) -> None:\n",
    "       \"\"\"Initialize with path to prompt template file.\"\"\"\n",
    "       self.template_path = template_path\n",
    "\n",
    "   def load_prompt(self) -> str:\n",
    "       \"\"\"Load prompt template from file.\"\"\"\n",
    "       with open(self.template_path, 'r') as file:\n",
    "           return file.read()\n",
    "\n",
    "   def format_prompt(self, country_name: str, scenario: str) -> str:\n",
    "       \"\"\"\n",
    "       Format prompt with country name and conversation scenario.\n",
    "       \n",
    "       Args:\n",
    "           country_name: Country Name\n",
    "           scenario: Conversation Scenario\n",
    "           \n",
    "       Returns:\n",
    "           Formatted prompt for LLM\n",
    "       \"\"\"\n",
    "       template = self.load_prompt()\n",
    "       return template.format(\n",
    "           country_name=country_name,\n",
    "           scenario=scenario\n",
    "       )\n",
    "\n",
    "class LLMAdapter:\n",
    "   \"\"\"Handles LLM interactions for text generation.\"\"\"\n",
    "   \n",
    "   def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\", temperature: float = 0.7) -> None:\n",
    "       \"\"\"Initialize with model configuration.\"\"\"\n",
    "       self.client = InferenceClient(api_key=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "       self.model_name = model_name\n",
    "       self.temperature = temperature\n",
    "\n",
    "   def generate_response(self, prompt: str) -> str:\n",
    "       \"\"\"\n",
    "       Generate complete response from LLM.\n",
    "       \n",
    "       Args:\n",
    "           prompt: Input text for LLM\n",
    "           \n",
    "       Returns:\n",
    "           Complete generated text\n",
    "       \"\"\"\n",
    "       try:\n",
    "           messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "           \n",
    "           completion = self.client.chat.completions.create(\n",
    "               model=self.model_name,\n",
    "               messages=messages,\n",
    "               temperature=self.temperature,\n",
    "               max_tokens=10000\n",
    "           )\n",
    "           \n",
    "           return completion.choices[0].message.content\n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Exception occurred: {str(e)}\")\n",
    "           return f\"Error: {str(e)}\"\n",
    "\n",
    "   async def generate_response_stream(self, prompt: str) -> AsyncGenerator[str, None]:\n",
    "       \"\"\"\n",
    "       Generate streaming response from LLM.\n",
    "       \n",
    "       Args:\n",
    "           prompt: Input text for LLM\n",
    "           \n",
    "       Returns:\n",
    "           AsyncGenerator yielding text chunks\n",
    "       \"\"\"\n",
    "       try:\n",
    "           messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "           \n",
    "           stream = self.client.chat.completions.create(\n",
    "               model=self.model_name,\n",
    "               messages=messages,\n",
    "               temperature=self.temperature,\n",
    "               max_tokens=10000,\n",
    "               stream=True\n",
    "           )\n",
    "           \n",
    "           for chunk in stream:\n",
    "               if chunk.choices[0].delta.content is not None:\n",
    "                   yield chunk.choices[0].delta.content\n",
    "                   await asyncio.sleep(0.01)  # small delay to control stream rate\n",
    "               \n",
    "       except Exception as e:\n",
    "           print(f\"Exception occurred in stream: {str(e)}\")\n",
    "           yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = \"Dominican Republic\" ## specify the country\n",
    "scenario  = \"A tourist enters a café in Punta Cana and orders some local breakfast and coffee.\"  ## specify the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_handler = PromptHandler(config.PROMPT_TEMPLATE_PATH)\n",
    "prompt = prompt_handler.format_prompt(country_name, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert content creator for language learners in Spanish. Your task is to generate a realistic conversation in the form of a transcript between two characters given the following contextual parameters:\n",
      "\n",
      "**Country where conversation takes place:**\n",
      "\n",
      "Dominican Republic\n",
      "\n",
      "**Specific scenario**\n",
      "\n",
      "A tourist enters a café in Punta Cana and orders some local breakfast and coffee.\n",
      "\n",
      "Ensure you generate a transcript that is clean and takes the dialect of the country in account. For example, when comparing Spanish spoken in Colombia versus Spanish spoken in Spain, \n",
      "there are certain differences between like the following: \n",
      "\n",
      "- If you want to say “You all are my best friends”, in Spain they would say “Vosotros sois mis mejores amigos” or “vosotras sois mis mejores amigas”. In Colombia they would say, “Ustedes son mis mejores amigos” \n",
      "or “ustedes son mis mejores amigas”. \n",
      "- If you want to say “Do you want to go out?”, in Spain they would say “¿Tenéis ganas de salir?” whereas in Colombia they would say “¿Tienen ganas de salir?\".\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_adapter = LLMAdapter(model_name=config.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:\n",
      "\n",
      "Café en Punta Cana, Dominican Republic\n",
      "\n",
      "(Tourist enters the café and looks at the menu)\n",
      "\n",
      "Tourist (pointing at the menu): ¡Qué delicioso todo parece! ¿Por qué, ¿qué es esto? (What does this look like? What is this?)\n",
      "\n",
      "Café Owner: Ah, ¡esto es un desayuno típico dominicano! Tiene mangu, queso frito, y una taza de café. (This is a typical Dominican breakfast! It has mangu, fried cheese, and a cup of coffee.)\n",
      "\n",
      "Tourist: ¡Perfecto! ¿Puedo pedirlo, por favor? (Perfect! Can I order it, please?)\n",
      "\n",
      "Café Owner: Claro, señor/señora. ¡Disfrútalo! (Of course, sir/madam. Enjoy it!)\n",
      "\n",
      "(Tourist sits down and drinks their coffee)\n",
      "\n",
      "Tourist: ¡Este café es delicioso! ¿Hay alguna cosa típica que recomiendes que trago con el desayuno? (This coffee is delicious! Is there something typical that you recommend I drink with the breakfast?)\n",
      "\n",
      "Café Owner: ¡Claro! ¡El mamajuana es una bebida tradicional dominicana que es perfecta con el desayuno! (Of course! The mamajuana is a traditional Dominican drink that is perfect with the breakfast!)\n",
      "\n",
      "Tourist: ¡Gracias por la recomendación! ¿Puedo ordenar una taza, por favor? (Thank you for the recommendation! Can I order a cup, please?)\n",
      "\n",
      "Café Owner: Claro, señor/señora. ¡Espero que disfrutes! (Of course, sir/madam. I hope you enjoy it!)\n",
      "\n",
      "Tourist: ¡Muchas gracias! (Thank you very much!)\n",
      "\n",
      "(Tourist finishes their meal and pays)\n",
      "\n",
      "Tourist: ¡Hasta luego! (See you later!)\n",
      "\n",
      "Café Owner: ¡Hasta luego, señor/señora! (See you later, sir/madam!)\n",
      "\n",
      "Notes: In the Dominican Republic, it is common to use \"señor\" and \"señora\" when addressing customers, even informally. Additionally, the \"vos\" form of the verb is not commonly used, and instead \"tú\" is used for informal situations.\n"
     ]
    }
   ],
   "source": [
    "response = llm_adapter.generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_convo():\n",
    "    async def convo_stream():\n",
    "        async for token in llm_adapter.generate_response_stream(prompt):\n",
    "            yield token\n",
    "    async for token in convo_stream():\n",
    "        print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Transcript:**\n",
      "\n",
      "Café Owner (CO): ¡Bienvenido! ¿Qué podemos preparar para desayunar hoy? (Welcome! What can we prepare for breakfast today?)\n",
      "\n",
      "Tourist (T): Hola, quisiera pedir un desayuno típico y un café. (Hello, I'd like to order a typical breakfast and a coffee.)\n",
      "\n",
      "CO: ¡Claro! Vamos a preparar un mangu, un tostón, un huevo frito y un café dominicano. (Of course! We'll prepare mangu, tostón, a fried egg, and a Dominican coffee for you.)\n",
      "\n",
      "T: ¡Gracias! ¿Y el precio, por favor? (Thank you! And the price, please?)\n",
      "\n",
      "CO: El precio es 200 pesos. (The price is 200 pesos.)\n",
      "\n",
      "T: Entiendo. ¿También tienes algo dulce para la mesa? (I understand. Do you have something sweet for the table too?)\n",
      "\n",
      "CO: ¡Sí! Tenemos medialunas y arroz con leche. (Yes, we have medialunas and rice pudding.)\n",
      "\n",
      "T: ¡Muy bien! Yo quiero una medialuna y una taza de arroz con leche. (Great! I'd like a medialuna and a cup of rice pudding.)\n",
      "\n",
      "CO: ¡Claro que sí! ¡Aquí está tu orden! (Of course! Here's your order!)\n",
      "\n",
      "T: ¡Muchas gracias! (Thank you very much!)\n",
      "\n",
      "CO: ¡De nada! ¡Vuelva pronto! (You're welcome! Come again soon!)\n",
      "\n",
      "T: Adiós. (Goodbye.)\n",
      "\n",
      "**Note:** In this transcript, the Dominican Spanish dialect was used, as per the contextual parameters provided."
     ]
    }
   ],
   "source": [
    "await stream_convo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
