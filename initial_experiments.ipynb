{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperLang Sample Concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample concept, we will generate transcripts in Spanish given the country of origin and scenario. For now, we manually specify the country and scenario in this notebook, but, in the future we can create more\n",
    "specific parameters (e.g. difficulty, number of speakers in conversation) that will be inputted by the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handles LLM interactions using Hugging Face's API for text generation.\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import Dict, Any, AsyncGenerator\n",
    "import asyncio\n",
    "import config\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class PromptHandler:\n",
    "   \"\"\"\n",
    "   Class to handle loading and formatting of prompt templates.\n",
    "   \"\"\"\n",
    "\n",
    "   def __init__(self, template_path: str) -> None:\n",
    "       \"\"\"Initialize with path to prompt template file.\"\"\"\n",
    "       self.template_path = template_path\n",
    "\n",
    "   def load_prompt(self) -> str:\n",
    "       \"\"\"Load prompt template from file.\"\"\"\n",
    "       with open(self.template_path, 'r') as file:\n",
    "           return file.read()\n",
    "\n",
    "   def format_prompt(self, country_name: str, scenario: str) -> str:\n",
    "       \"\"\"\n",
    "       Format prompt with country name and conversation scenario.\n",
    "\n",
    "       Args:\n",
    "           country_name: Country Name\n",
    "           scenario: Conversation Scenario\n",
    "\n",
    "       Returns:\n",
    "           Formatted prompt for LLM\n",
    "       \"\"\"\n",
    "       template = self.load_prompt()\n",
    "       return template.format(\n",
    "           country_name=country_name,\n",
    "           scenario=scenario\n",
    "       )\n",
    "   def format_prompt_scenario(self, scenario: str) -> str:\n",
    "       \"\"\"\n",
    "       Format prompt with conversation scenario.\n",
    "\n",
    "       Args:\n",
    "           scenario: Conversation Scenario\n",
    "\n",
    "       Returns:\n",
    "           Formatted prompt for LLM\n",
    "       \"\"\"\n",
    "       template = self.load_prompt()\n",
    "       return template.format(scenario=scenario)\n",
    "\n",
    "\n",
    "class LLMAdapter:\n",
    "   \"\"\"Handles LLM interactions for text generation.\"\"\"\n",
    "\n",
    "   def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\", temperature: float = 0.7) -> None:\n",
    "       \"\"\"Initialize with model configuration.\"\"\"\n",
    "       self.client = InferenceClient(api_key=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "       self.model_name = model_name\n",
    "       self.temperature = temperature\n",
    "\n",
    "   def generate_response(self, prompt: str) -> str:\n",
    "       \"\"\"\n",
    "       Generate complete response from LLM.\n",
    "\n",
    "       Args:\n",
    "           prompt: Input text for LLM\n",
    "\n",
    "       Returns:\n",
    "           Complete generated text\n",
    "       \"\"\"\n",
    "       try:\n",
    "           messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "           completion = self.client.chat.completions.create(\n",
    "               model=self.model_name,\n",
    "               messages=messages,\n",
    "               temperature=self.temperature,\n",
    "               max_tokens=10000\n",
    "           )\n",
    "\n",
    "           return completion.choices[0].message.content\n",
    "\n",
    "       except Exception as e:\n",
    "           print(f\"Exception occurred: {str(e)}\")\n",
    "           return f\"Error: {str(e)}\"\n",
    "\n",
    "   async def generate_response_stream(self, prompt: str) -> AsyncGenerator[str, None]:\n",
    "       \"\"\"\n",
    "       Generate streaming response from LLM.\n",
    "\n",
    "       Args:\n",
    "           prompt: Input text for LLM\n",
    "\n",
    "       Returns:\n",
    "           AsyncGenerator yielding text chunks\n",
    "       \"\"\"\n",
    "       try:\n",
    "           messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "           stream = self.client.chat.completions.create(\n",
    "               model=self.model_name,\n",
    "               messages=messages,\n",
    "               temperature=self.temperature,\n",
    "               max_tokens=10000,\n",
    "               stream=True\n",
    "           )\n",
    "\n",
    "           for chunk in stream:\n",
    "               if chunk.choices[0].delta.content is not None:\n",
    "                   yield chunk.choices[0].delta.content\n",
    "                   await asyncio.sleep(0.01)  # small delay to control stream rate\n",
    "\n",
    "       except Exception as e:\n",
    "           print(f\"Exception occurred in stream: {str(e)}\")\n",
    "           yield f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name = \"Colombia\" ## specify the country\n",
    "scenario  = \"A tourist enters a café in Punta Cana and orders some breakfast and coffee.\"  ## specify the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A guest arrives at a hotel and interacts with the receptionist to check in.\n",
      "The receptionist welcomes the guest and confirms their reservation.\n",
      "They provide details about the room, including its type and amenities (a suite with a sea view and a double bed).\n",
      "The guest asks if there are any discounts for frequent members, but the receptionist explains that no discounts are available.\n",
      "However, they offer a late check-out as a courtesy.\n",
      "\n",
      "The receptionist then hands over the room key and asks about the guest's luggage.\n",
      "Upon confirming that the guest has two large suitcases, they call a porter to assist with carrying the luggage to the room.\n",
      "The conversation concludes with the receptionist offering assistance if needed and warmly welcoming the guest to enjoy their stay.\n"
     ]
    }
   ],
   "source": [
    "scenario_path = \"./data/scenarios/hotel.txt\"\n",
    "with open(scenario_path, 'r') as file:\n",
    "    scenario = file.read()\n",
    "print(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.PROMPT_TEMPLATE_PATH)\n",
    "prompt_handler = PromptHandler(config.PROMPT_TEMPLATE_PATH)\n",
    "prompt = prompt_handler.format_prompt_scenario(country_name, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For prompts that only require scenarios (no country specification)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mPROMPT_TEMPLATE_SCENARIO_ONLY_PATH)\n\u001b[1;32m      3\u001b[0m prompt_handler \u001b[38;5;241m=\u001b[39m PromptHandler(config\u001b[38;5;241m.\u001b[39mPROMPT_TEMPLATE_SCENARIO_ONLY_PATH)\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_handler\u001b[38;5;241m.\u001b[39mformat_prompt_scenario(scenario)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# For prompts that only require scenarios (no country specification)\n",
    "print(config.PROMPT_TEMPLATE_SCENARIO_ONLY_PATH)\n",
    "prompt_handler = PromptHandler(config.PROMPT_TEMPLATE_SCENARIO_ONLY_PATH)\n",
    "prompt = prompt_handler.format_prompt_scenario(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert content creator for language lessons in Spanish. Your task is to generate a realistic conversation in the form of a transcript.\n",
      "The main character is the language learner. Make it as realistic as possible.\n",
      "Only include diaglogue and no narration or context setting.\n",
      "Use following contextual parameters:\n",
      "**Specific scenario**\n",
      "A guest arrives at a hotel and interacts with the receptionist to check in.\n",
      "The receptionist welcomes the guest and confirms their reservation.\n",
      "They provide details about the room, including its type and amenities (a suite with a sea view and a double bed).\n",
      "The guest asks if there are any discounts for frequent members, but the receptionist explains that no discounts are available.\n",
      "However, they offer a late check-out as a courtesy.\n",
      "\n",
      "The receptionist then hands over the room key and asks about the guest's luggage.\n",
      "Upon confirming that the guest has two large suitcases, they call a porter to assist with carrying the luggage to the room.\n",
      "The conversation concludes with the receptionist offering assistance if needed and warmly welcoming the guest to enjoy their stay.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_adapter = LLMAdapter(model_name=config.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Learner (LL): ¿Tiene usted la hora, por favor? (Do you have the time, please?)\n",
      "\n",
      "Stranger (S): Sí, es 16:50. (Yes, it's 4:50 PM.)\n",
      "\n",
      "LL: ¡Gracias! ¿Me podría decir dónde está la estación de tren de Madrid? (Thank you! Could you tell me where the Madrid train station is?)\n",
      "\n",
      "S: Claro, sigue la callePríncipe de Vergara hasta el final, y por allí verás la estación de tren de Madrid. (Sure, follow Príncipe de Vergara street to the end, and there you will see the Madrid train station.)\n",
      "\n",
      "LL: ¡Muchas gracias por su ayuda! (Thank you very much for your help!)\n",
      "\n",
      "S: De nada, buena suerte en tu viaje. (You're welcome, good luck on your journey.)\n",
      "\n",
      "LL: Necesito llegar antes de las 17:30. (I need to get there before 5:30 PM.)\n",
      "\n",
      "S: ¡Espero que logres llegar a tiempo! (I hope you make it on time!)\n",
      "\n",
      "LL: ¡Claro! Me despido. (Of course! Goodbye.)\n",
      "\n",
      "S: Adiós. (Goodbye.)\n"
     ]
    }
   ],
   "source": [
    "response = llm_adapter.generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_convo():\n",
    "    async def convo_stream():\n",
    "        async for token in llm_adapter.generate_response_stream(prompt):\n",
    "            yield token\n",
    "    async for token in convo_stream():\n",
    "        print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Receptionist: ¡Bienvenido! ¿Cómo estás? (Welcome! How are you?)\n",
      "\n",
      "Guest: Hola, estoy bien, gracias. ¿Soy el señor...? (Hi, I'm fine, thank you. Am I Mr...?)\n",
      "\n",
      "Receptionist: ¡Claro! Estás aquí por reserva 345, sí? (Yes, you are here for reservation 345, right?)\n",
      "\n",
      "Guest: Sí, eso es. ¿En qué tipo de habitación me he reservado? (Yes, that's correct. In what type of room have I reserved?)\n",
      "\n",
      "Receptionist: Te hemos asignado una suite con vista al mar y una cama matrimonial. (We have assigned you a suite with a sea view and a double bed.)\n",
      "\n",
      "Guest: ¡Muy bien! ¿Hay algún descuento para miembros frecuentes? (Very good! Is there any discount for frequent members?)\n",
      "\n",
      "Receptionist: Lo siento, no hay descuentos disponibles en este momento. Sin embargo, podemos ofrecerte una salida tardía como gesto de cortesía. (I'm sorry, there are no discounts available at the moment. However, we can offer you a late check-out as a courtesy.)\n",
      "\n",
      "Guest: Gracias, eso es muy amable. ¿Tuve que pagar por esto adicionalmente? (Thank you, that's very kind. Did I have to pay extra for this?)\n",
      "\n",
      "Receptionist: No, ¡no es necesario! Tiene incluida en su reserva. Ahora, ¿tienes dos maletas grandes? (No, it's included in your reservation. Now, do you have two large suitcases?)\n",
      "\n",
      "Guest: Sí, eso es. ¿Puedo llamar a un portador para ayudarme con ellos? (Yes, that's correct. Can I call a porter to help me with them?)\n",
      "\n",
      "Receptionist: Claro, ¡por supuesto! Llamaré a un portador para ayudarte. (Of course, absolutely! I'll call a porter to help you.)\n",
      "\n",
      "Guest: ¡Muchas gracias! ¿Qué puedo hacer si tengo alguna pregunta más o necesito ayuda adicional? (Thank you! What can I do if I have more questions or need additional help?)\n",
      "\n",
      "Receptionist: Siempre estamos aquí para ayudarte. No dude en preguntar o pedir ayuda si es necesario. ¡Disfrute su estancia y disfrute de la suite! (We are always here to help you. Don't hesitate to ask or ask for help if needed. Enjoy your stay and enjoy the suite!)"
     ]
    }
   ],
   "source": [
    "await stream_convo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
